{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c27cfd69ce134a3282162792666afc75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e4d425401254c49adeef799c051f793",
              "IPY_MODEL_1b517172a0d24bf4afb466ce38bc5026",
              "IPY_MODEL_600ccf4c62ae4553ad8cf40ee20b658c"
            ],
            "layout": "IPY_MODEL_4c644543049a42eb9167d53ed25593f3"
          }
        },
        "6e4d425401254c49adeef799c051f793": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d75b7f3ed57b4956adc617e3bc457236",
            "placeholder": "​",
            "style": "IPY_MODEL_e634ce7104224923ae57a83d1f42ee60",
            "value": "100%"
          }
        },
        "1b517172a0d24bf4afb466ce38bc5026": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_320ba5fceecc490982561436bf00f60c",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe51b41329024b33a3036b11462a55d2",
            "value": 1000
          }
        },
        "600ccf4c62ae4553ad8cf40ee20b658c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb162ca75ffd46d69b5d2ee3090d99ad",
            "placeholder": "​",
            "style": "IPY_MODEL_ea8d8d79723e45fabbe3564a37b6a900",
            "value": " 1000/1000 [01:50&lt;00:00,  6.47it/s, distance=7.3]"
          }
        },
        "4c644543049a42eb9167d53ed25593f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d75b7f3ed57b4956adc617e3bc457236": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e634ce7104224923ae57a83d1f42ee60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "320ba5fceecc490982561436bf00f60c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe51b41329024b33a3036b11462a55d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb162ca75ffd46d69b5d2ee3090d99ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea8d8d79723e45fabbe3564a37b6a900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This project is based on Diffusion Posterior Sampling implementation (https://github.com/DPS2022/diffusion-posterior-sampling.git), to leverage the structure of the code with respect to Guided Diffusion (https://github.com/openai/guided-diffusion.git) and ILVR-ADM (https://github.com/jychoi118/ilvr_adm.git)\n",
        "Both ILVR-ADM and DPS are re-implementations of Guided Diffusion.\n",
        "\n",
        "I have focused on MPGD without Projection on the task of Super Resolution."
      ],
      "metadata": {
        "id": "GmfKR_xxxjVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The full code can be found here:"
      ],
      "metadata": {
        "id": "lN7gwcR0ToAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/giorgiacucino/MPGD.git"
      ],
      "metadata": {
        "id": "MvvqHNCmTnmN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2217740c-3208-4b90-d742-ecad1c3a7de7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MPGD'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 46 (delta 9), reused 42 (delta 7), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (46/46), 16.18 MiB | 15.41 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of MPGD shortcut"
      ],
      "metadata": {
        "id": "ru7Dj7lZIfQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement Manifold Preserving Guided Diffusion, we have to re-implement the loop that denoises the initial full noise sample e.g. the loop from x_t to x_0"
      ],
      "metadata": {
        "id": "AzKz9vBZI5gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MPGD(SpacedDiffusion):\n",
        "    def p_sample_loop(self,\n",
        "                      model,\n",
        "                      x_start,\n",
        "                      measurement,\n",
        "                      measurement_cond_fn,\n",
        "                      record,\n",
        "                      save_root):\n",
        "        img = x_start\n",
        "        device = x_start.device\n",
        "\n",
        "        pbar = tqdm(list(range(self.num_timesteps))[::-1])\n",
        "        for idx in pbar:\n",
        "            time = torch.tensor([idx] * img.shape[0], device=device)\n",
        "            #Predicts the model output e.g. eps_theta(x_t, t)\n",
        "            model_output = model(img, self._scale_timesteps(time))\n",
        "\n",
        "            # In the case of \"learned\" variance, model will give twice channels.\n",
        "            if model_output.shape[1] == 2 * img.shape[1]:\n",
        "                model_output, model_var_values = torch.split(model_output, img.shape[1], dim=1)\n",
        "            else:\n",
        "                model_var_values = model_output\n",
        "\n",
        "            #Predicts x_0|t\n",
        "            pred_xstart = self.mean_processor.predict_xstart(x_t=img, t=time, eps=model_output)\n",
        "\n",
        "            #Clone x_0|t to require grad on it\n",
        "            x_0_t = pred_xstart.clone()\n",
        "            x_0_t = x_0_t.requires_grad_()\n",
        "\n",
        "            #Calculate the scaling factor c_t based on the alpha_bar and alpha_bar_prev\n",
        "            #This implementation is based on the formula at page 17 of the paper\n",
        "            alpha_bar = extract_and_expand(self.alphas_cumprod, time, img)\n",
        "            alpha_bar_prev = extract_and_expand(self.alphas_cumprod_prev, time, img)\n",
        "            scale = 1.0 / (torch.sqrt(alpha_bar * alpha_bar_prev))\n",
        "\n",
        "            #Calculates the gradient with respect to x_0_t and updates the predicted x_start\n",
        "            #using the measurement_cond_fn, which can be set using a configuration file.\n",
        "            #(see \"Conditioning method\" section for more details)\n",
        "            pred_xstart, distance = measurement_cond_fn(x_t=pred_xstart,\n",
        "                                                  measurement=measurement,\n",
        "                                                  x_prev=x_0_t,\n",
        "                                                  x_0_hat=x_0_t,\n",
        "                                                  idx=idx,\n",
        "                                                  timesteps=self.num_timesteps,\n",
        "                                                  scale=scale)\n",
        "            x_0_t = x_0_t.detach_()\n",
        "            with torch.no_grad():\n",
        "                #Calculates sigma with eta=0.5 (as in the implementation of the paper)\n",
        "                sigma = (0.5 * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar)) * torch.sqrt(1 - alpha_bar / alpha_bar_prev))\n",
        "                noise = torch.randn_like(img)\n",
        "                #Calculates x_t = x_0|t * sqrt(alpha_bar_prev) + sqrt(1 - alpha_bar_prev - (sigma ** 2)) * model_output\n",
        "                #(as in the implementation of the paper)\n",
        "                img = (pred_xstart * torch.sqrt(alpha_bar_prev) + torch.sqrt(1 - alpha_bar_prev - (sigma ** 2)) * model_output)\n",
        "                if time != 0:\n",
        "                    img += sigma * noise\n",
        "            pbar.set_postfix({'distance': distance.item()}, refresh=False)\n",
        "            if record:\n",
        "                if idx % 10 == 0:\n",
        "                    #Outputs files for debugging purposes (see in \"Results\" section for more details)\n",
        "                    file_path = os.path.join(save_root, f\"progress/img_{str(idx).zfill(4)}.png\")\n",
        "                    file_path2 = os.path.join(save_root, f\"progress/x_{str(idx).zfill(4)}.png\")\n",
        "                    file_path3 = os.path.join(save_root, f\"progress/eps_{str(idx).zfill(4)}.png\")\n",
        "                    file_path4 = os.path.join(save_root, f\"progress/pred_{str(idx).zfill(4)}.png\")\n",
        "                    plt.imsave(file_path, clear_color(img))\n",
        "                    plt.imsave(file_path2, clear_color(x_0_t))\n",
        "                    plt.imsave(file_path3, clear_color(model_output))\n",
        "                    plt.imsave(file_path4, clear_color(pred_xstart))\n",
        "\n",
        "        return img"
      ],
      "metadata": {
        "id": "xAIl-d-OIq1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conditioning Method"
      ],
      "metadata": {
        "id": "N9_3nXANSVCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The measurement_cond_fn is set to this in the configuration file \"configs/mpgd_super_resolution_config.yaml\""
      ],
      "metadata": {
        "id": "fztgUSHKTOe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@register_conditioning_method(name='mpg')\n",
        "class ManifoldPreservingGradient(ConditioningMethod):\n",
        "    def __init__(self, operator, noiser, **kwargs):\n",
        "        super().__init__(operator, noiser)\n",
        "        self.scale = kwargs.get('scale', 1.0)\n",
        "\n",
        "    def conditioning(self, x_prev, x_t, x_0_hat, measurement, idx, timesteps, **kwargs):\n",
        "        #Calculate the gradient and the norm\n",
        "        norm_grad, norm = self.grad_and_value(x_prev=x_prev, x_0_hat=x_0_hat, measurement=measurement, **kwargs)\n",
        "        x_t -= norm_grad * self.scale\n",
        "\n",
        "        #Project the data in the first half of the sampling process (different from the paper)\n",
        "        if (idx > timesteps/2):\n",
        "          x_t = self.project(data=x_t, noisy_measurement=measurement, **kwargs)\n",
        "        return x_t, norm"
      ],
      "metadata": {
        "id": "yBO5jYA-SqTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is different from the paper because after the update of the clean data estimation I project it.\n",
        "\n",
        "I did this because empirically I found out that:\n",
        "\n",
        "1.   if the data is not projected, it tends to diverge from the original input\n",
        "2.   if the data is projected for the whole process, it tends to preserve some noise from the measurement\n",
        "\n",
        "Moreover, projecting for the whole process has a negative impact on the model output calculated on the next iteration"
      ],
      "metadata": {
        "id": "9kdcPI_QVNq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to run the code"
      ],
      "metadata": {
        "id": "f7xbXlf-X0li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, you will need to clone the repository with the code from github"
      ],
      "metadata": {
        "id": "pT_LoEan-47X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/giorgiacucino/MPGD.git"
      ],
      "metadata": {
        "id": "7tYpNZMNYg4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c2df47-5d0d-4c72-c45b-e876f77bbbc1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MPGD'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 46 (delta 9), reused 42 (delta 7), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (46/46), 16.18 MiB | 17.02 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, create a ./models directory to store the pretrained model"
      ],
      "metadata": {
        "id": "NXudztHarxnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/MPGD"
      ],
      "metadata": {
        "id": "sDRZBEdWrxI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3000256-2005-4988-f708-c2f3b4b079ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MPGD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir /content/MPGD/models"
      ],
      "metadata": {
        "id": "vpfrY2H3s4sN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can get the pretrained model from this [link](https://drive.google.com/drive/folders/1jElnRoFv7b31fG0v6pTSQkelbSX3xGZh)\n",
        "\n",
        "If the model is saved on Google Drive, run the next cell"
      ],
      "metadata": {
        "id": "AKRiRuJms_hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkfwVwMGs-7V",
        "outputId": "6de492d6-6e23-4745-a2fe-17f7352e6eb5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/ffhq_10m.pt /content/MPGD/models"
      ],
      "metadata": {
        "id": "jzv64oAVMDMl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/VinAIResearch/blur-kernel-space-exploring bkse\n",
        "\n",
        "!git clone https://github.com/LeviBorodenko/motionblur motionblur"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVpDjiAWMBCr",
        "outputId": "5b3b60b7-d9a5-40c1-fbea-eb7c067f2ca5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bkse'...\n",
            "remote: Enumerating objects: 565, done.\u001b[K\n",
            "remote: Counting objects: 100% (565/565), done.\u001b[K\n",
            "remote: Compressing objects: 100% (316/316), done.\u001b[K\n",
            "remote: Total 565 (delta 327), reused 461 (delta 232), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (565/565), 1.04 MiB | 10.88 MiB/s, done.\n",
            "Resolving deltas: 100% (327/327), done.\n",
            "Cloning into 'motionblur'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Total 36 (delta 0), reused 0 (delta 0), pack-reused 36\u001b[K\n",
            "Receiving objects: 100% (36/36), 511.08 KiB | 5.16 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "bRmqQqFJVM-B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "#From DPS implementation (sample_condition.py)\n",
        "def load_yaml(file_path: str) -> dict:\n",
        "    with open(file_path) as f:\n",
        "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
        "    return config\n",
        "\n",
        "#Change the configuration files here\n",
        "model_config = load_yaml(\"./configs/model_config.yaml\")\n",
        "diffusion_config = load_yaml(\"./configs/mpgd_diffusion_config.yaml\")\n",
        "task_config = load_yaml(\"./configs/mpgd_super_resolution_config.yaml\")\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from functools import partial\n",
        "import os\n",
        "\n",
        "from guided_diffusion.condition_methods import get_conditioning_method\n",
        "from guided_diffusion.measurements import get_noise, get_operator\n",
        "from guided_diffusion.unet import create_model\n",
        "from guided_diffusion.gaussian_diffusion import create_sampler\n",
        "from data.dataloader import get_dataset, get_dataloader\n",
        "from util.img_utils import clear_color, mask_generator\n",
        "from util.logger import get_logger\n",
        "\n",
        "def start_sampling(model_config, diffusion_config, task_config, device):\n",
        "    # Load model\n",
        "    model = create_model(**model_config)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare Operator and noise\n",
        "    measure_config = task_config['measurement']\n",
        "    operator = get_operator(device=device, **measure_config['operator'])\n",
        "    noiser = get_noise(**measure_config['noise'])\n",
        "\n",
        "    # Prepare conditioning method\n",
        "    cond_config = task_config['conditioning']\n",
        "    cond_method = get_conditioning_method(cond_config['method'], operator, noiser, **cond_config['params'])\n",
        "    measurement_cond_fn = cond_method.conditioning\n",
        "\n",
        "    # Load diffusion sampler\n",
        "    sampler = create_sampler(**diffusion_config)\n",
        "    sample_fn = partial(sampler.p_sample_loop, model=model, measurement_cond_fn=measurement_cond_fn)\n",
        "\n",
        "    # Working directory\n",
        "    out_path = os.path.join(\"./results\", measure_config['operator']['name'])\n",
        "    os.makedirs(out_path, exist_ok=True)\n",
        "    for img_dir in ['input', 'recon', 'progress', 'label']:\n",
        "        os.makedirs(os.path.join(out_path, img_dir), exist_ok=True)\n",
        "\n",
        "    # Prepare dataloader\n",
        "    data_config = task_config['data']\n",
        "    transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    dataset = get_dataset(**data_config, transforms=transform)\n",
        "    loader = get_dataloader(dataset, batch_size=1, num_workers=0, train=False)\n",
        "\n",
        "    # Exception) In case of inpainting, we need to generate a mask\n",
        "    if measure_config['operator']['name'] == 'inpainting':\n",
        "        mask_gen = mask_generator(\n",
        "            **measure_config['mask_opt']\n",
        "        )\n",
        "\n",
        "    # Do Inference\n",
        "    for i, ref_img in enumerate(loader):\n",
        "        fname = str(i).zfill(5) + '.png'\n",
        "        ref_img = ref_img.to(device)\n",
        "\n",
        "        # Exception) In case of inpainging,\n",
        "        if measure_config['operator'] ['name'] == 'inpainting':\n",
        "            mask = mask_gen(ref_img)\n",
        "            mask = mask[:, 0, :, :].unsqueeze(dim=0)\n",
        "            measurement_cond_fn = partial(cond_method.conditioning, mask=mask)\n",
        "            sample_fn = partial(sample_fn, measurement_cond_fn=measurement_cond_fn)\n",
        "\n",
        "            # Forward measurement model (Ax + n)\n",
        "            y = operator.forward(ref_img, mask=mask)\n",
        "            y_n = noiser(y)\n",
        "\n",
        "        else:\n",
        "            # Forward measurement model (Ax + n)\n",
        "            y = operator.forward(ref_img)\n",
        "            y_n = noiser(y)\n",
        "\n",
        "        # Sampling\n",
        "        x_start = torch.randn(ref_img.shape, device=device).requires_grad_()\n",
        "        sample = sample_fn(x_start=x_start, measurement=y_n, record=True, save_root=out_path)\n",
        "\n",
        "        plt.imsave(os.path.join(out_path, 'input', fname), clear_color(y_n))\n",
        "        plt.imsave(os.path.join(out_path, 'label', fname), clear_color(ref_img))\n",
        "        plt.imsave(os.path.join(out_path, 'recon', fname), clear_color(sample))"
      ],
      "metadata": {
        "id": "lnqi2oGm_nqy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_sampling(model_config, diffusion_config, task_config, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c27cfd69ce134a3282162792666afc75",
            "6e4d425401254c49adeef799c051f793",
            "1b517172a0d24bf4afb466ce38bc5026",
            "600ccf4c62ae4553ad8cf40ee20b658c",
            "4c644543049a42eb9167d53ed25593f3",
            "d75b7f3ed57b4956adc617e3bc457236",
            "e634ce7104224923ae57a83d1f42ee60",
            "320ba5fceecc490982561436bf00f60c",
            "fe51b41329024b33a3036b11462a55d2",
            "bb162ca75ffd46d69b5d2ee3090d99ad",
            "ea8d8d79723e45fabbe3564a37b6a900"
          ]
        },
        "id": "IRi57TEcSXvf",
        "outputId": "c347f2ee-71f7-4755-899c-d73b04b41495"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c27cfd69ce134a3282162792666afc75"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "RYHCGKwYY5PG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize the results, I have created a function that generates the gifs for some of the data used in the loop.\n",
        "\n",
        "The result can be found in the gif img.gif"
      ],
      "metadata": {
        "id": "awOk7zAMY4zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "import imageio\n",
        "\n",
        "def generate_gif(image_folder, output_gif_path, start):\n",
        "    image_files = [f for f in os.listdir(image_folder) if f.startswith(start) and f.endswith(\".png\")]\n",
        "\n",
        "    image_files.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
        "\n",
        "    images = []\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(image_folder, image_file)\n",
        "        image = Image.open(image_path)\n",
        "        images.append(image)\n",
        "\n",
        "    images = images[::-1]\n",
        "    gif_path = output_gif_path\n",
        "    imageio.mimsave(gif_path, images, duration=0.5)\n",
        "\n",
        "input_folder = \"./results/super_resolution/progress\"\n",
        "\n",
        "!mkdir \"./gifs\"\n",
        "\n",
        "output_folder = \"./gifs/\"\n",
        "x_0 = \"x_0.gif\"\n",
        "img = \"img.gif\"\n",
        "pred = \"pred.gif\"\n",
        "eps = \"eps.gif\"\n",
        "\n",
        "generate_gif(input_folder, output_folder + x_0, \"x_\")\n",
        "generate_gif(input_folder, output_folder + img, \"img_\")\n",
        "generate_gif(input_folder, output_folder + pred, \"pred_\")\n",
        "generate_gif(input_folder, output_folder + eps, \"eps_\")\n"
      ],
      "metadata": {
        "id": "jk8XwLtLo-W9"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}